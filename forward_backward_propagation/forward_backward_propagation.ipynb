{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set manual seed\n",
    "torch.manual_seed(2)\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "iris = datasets.load_iris()\n",
    "X = torch.tensor(preprocessing.normalize(iris.data[:, :2]), dtype=torch.float)\n",
    "y = torch.tensor(iris.target.reshape(-1, 1), dtype=torch.float)\n",
    "\n",
    "# We only take 2 classes to make a binary classification problem\n",
    "X = X[:y[y < 2].size()[0]]\n",
    "y = y[:y[y < 2].size()[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Dimensions for input, hidden and output\n",
    "        self.input_dim = 2\n",
    "        self.hidden_dim = 32\n",
    "        self.output_dim = 1\n",
    "\n",
    "        # Learning rate definition\n",
    "        self.learning_rate = 0.001\n",
    "\n",
    "        # Our parameters (weights)\n",
    "        # w1: 2 x 32\n",
    "        self.w1 = torch.randn(self.input_dim, self.hidden_dim)\n",
    "\n",
    "        # w2: 32 x 1 ##outputdim--> hiddendim\n",
    "        ##self.w2 = torch.randn(self.hidden_dim, self.output_dim)\n",
    "        self.w2 = torch.randn(self.hidden_dim, self.hidden_dim)\n",
    "        \n",
    "        ##LAYER 하나 추가\n",
    "        ## w3\n",
    "        self.w3 = torch.randn(self.hidden_dim, self.output_dim)\n",
    "\n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1 + torch.exp(-s))\n",
    "\n",
    "    def sigmoid_first_order_derivative(self, s):\n",
    "        return s * (1 - s)\n",
    "\n",
    "    # Forward propagation\n",
    "    def forward(self, X):\n",
    "        # First linear layer\n",
    "        self.y1 = torch.matmul(X, self.w1) # 3 X 3 \".dot\" does not broadcast in PyTorch\n",
    "\n",
    "        # First non-linearity\n",
    "        self.y2 = self.sigmoid(self.y1)\n",
    "\n",
    "        # Second linear layer\n",
    "        self.y3 = torch.matmul(self.y2, self.w2)\n",
    "\n",
    "        # Second non-linearity\n",
    "        ##y4를 self.y4로 \n",
    "        self.y4 = self.sigmoid(self.y3)\n",
    "        \n",
    "        ## Third linear layer\n",
    "        self.y5 = torch.matmul(self.y4, self.w3)\n",
    "        \n",
    "        ## Third non-linearity\n",
    "        y6 = self.sigmoid(self.y5)\n",
    "        \n",
    "        ##y4 --> y6\n",
    "        return y6\n",
    "    \n",
    "    ##y4-->y6\n",
    "    # Backward propagation\n",
    "    def backward(self, X, l, y6):\n",
    "        # Derivative of binary cross entropy cost w.r.t. final output y4\n",
    "        ##y4-->y6\n",
    "        self.dC_dy6 = y6 - l\n",
    "        \n",
    "    \n",
    "\n",
    "        ##\n",
    "        '''\n",
    "        Gradients for w3: partial derivative of cost w.r.t. w2\n",
    "        dC/dw2\n",
    "        '''\n",
    "        ##y4-->y6\n",
    "        self.dy6_dy5 = self.sigmoid_first_order_derivative(y6)\n",
    "        self.dy5_dw3 = self.y4\n",
    "        \n",
    "        ##y4-->y6\n",
    "        # Y4 delta: dC_dy4 dy4_dy3\n",
    "        self.y6_delta = self.dC_dy6 * self.dy6_dy5\n",
    "        \n",
    "        ##y4-->y6\n",
    "        # This is our gradients for w1: dC_dy4 dy4_dy3 dy3_dw2\n",
    "        self.dC_dw3 = torch.matmul(torch.t(self.dy5_dw3), self.y6_delta)\n",
    "\n",
    "        \n",
    "        '''\n",
    "        Gradients for w2: partial derivative of cost w.r.t. w2\n",
    "        dC/dw2\n",
    "        '''\n",
    "        self.dy5_dy4 = self.w3\n",
    "        self.dy4_dy3 = self.sigmoid_first_order_derivative(self.y4)\n",
    "        self.dy3_dw2 = self.y2\n",
    "\n",
    "        # Y2 delta: (dC_dy4 dy4_dy3) dy3_dy2 dy2_dy1\n",
    "        self.y4_delta = torch.matmul(self.y6_delta, torch.t(self.dy5_dy4)) * self.dy4_dy3\n",
    "\n",
    "        # Gradients for w1: (dC_dy4 dy4_dy3) dy3_dy2 dy2_dy1 dy1_dw1\n",
    "        self.dC_dw2 = torch.matmul(torch.t(self.dy3_dw2), self.y4_delta)\n",
    "\n",
    "        '''\n",
    "        Gradients for w1: partial derivative of cost w.r.t w1\n",
    "        dC/dw1\n",
    "        '''\n",
    "        self.dy3_dy2 = self.w2\n",
    "        self.dy2_dy1 = self.sigmoid_first_order_derivative(self.y2)\n",
    "\n",
    "        # Y2 delta: (dC_dy4 dy4_dy3) dy3_dy2 dy2_dy1\n",
    "        self.y2_delta = torch.matmul(self.y4_delta, torch.t(self.dy3_dy2)) * self.dy2_dy1\n",
    "\n",
    "        # Gradients for w1: (dC_dy4 dy4_dy3) dy3_dy2 dy2_dy1 dy1_dw1\n",
    "        self.dC_dw1 = torch.matmul(torch.t(X), self.y2_delta)\n",
    "\n",
    "        # Gradient descent on the weights from our 2 linear layers\n",
    "        self.w1 -= self.learning_rate * self.dC_dw1\n",
    "        self.w2 -= self.learning_rate * self.dC_dw2\n",
    "        self.w3 -= self.learning_rate * self.dC_dw3\n",
    "\n",
    "    def train(self, X, l):\n",
    "        # Forward propagation\n",
    "        y6 = self.forward(X)\n",
    "\n",
    "        # Backward propagation and gradient descent\n",
    "        self.backward(X, l, y6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 0.7343553304672241\n",
      "Epoch 20 | Loss: 0.6719828844070435\n",
      "Epoch 40 | Loss: 0.6701794266700745\n",
      "Epoch 60 | Loss: 0.6685028076171875\n",
      "Epoch 80 | Loss: 0.6668243408203125\n",
      "Epoch 100 | Loss: 0.6651425361633301\n"
     ]
    }
   ],
   "source": [
    "# Instantiate our model class and assign it to our model object\n",
    "model = FNN()\n",
    "\n",
    "# Loss list for plotting of loss behaviour\n",
    "loss_lst = []\n",
    "\n",
    "# Number of times we want our FNN to look at all 100 samples we have, 100 implies looking through 100x\n",
    "num_epochs = 101\n",
    "\n",
    "# Let's train our model with 100 epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Get our predictions\n",
    "    y_hat = model(X)\n",
    "\n",
    "    # Cross entropy loss, remember this can never be negative by nature of the equation\n",
    "    # But it does not mean the loss can't be negative for other loss functions\n",
    "    cross_entropy_loss = -(y * torch.log(y_hat) + (1 - y) * torch.log(1 - y_hat))\n",
    "\n",
    "    # We have to take cross entropy loss over all our samples, 100 in this 2-class iris dataset\n",
    "    mean_cross_entropy_loss = torch.mean(cross_entropy_loss).detach().item()\n",
    "\n",
    "    # Print our mean cross entropy loss\n",
    "    if epoch % 20 == 0:\n",
    "        print('Epoch {} | Loss: {}'.format(epoch, mean_cross_entropy_loss))\n",
    "    loss_lst.append(mean_cross_entropy_loss)\n",
    "\n",
    "    # (1) Forward propagation: to get our predictions to pass to our cross entropy loss function\n",
    "    # (2) Back propagation: get our partial derivatives w.r.t. parameters (gradients)\n",
    "    # (3) Gradient Descent: update our weights with our gradients\n",
    "    model.train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
